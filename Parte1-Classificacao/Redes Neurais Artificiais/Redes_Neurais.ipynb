{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redes Neurais Artificiais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base Credit Data - 99,8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../credit.pkl', 'rb') as f:\n",
    "    x_credit_treinamento, y_credit_treinamento, x_credit_teste, y_credit_teste = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1500, 3), (1500,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_credit_treinamento.shape, y_credit_treinamento.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 3), (500,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_credit_teste.shape, y_credit_teste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.38528568\n",
      "Iteration 2, loss = 0.38407917\n",
      "Iteration 3, loss = 0.38295583\n",
      "Iteration 4, loss = 0.38164278\n",
      "Iteration 5, loss = 0.38042500\n",
      "Iteration 6, loss = 0.37921846\n",
      "Iteration 7, loss = 0.37799810\n",
      "Iteration 8, loss = 0.37673765\n",
      "Iteration 9, loss = 0.37545917\n",
      "Iteration 10, loss = 0.37419061\n",
      "Iteration 11, loss = 0.37290691\n",
      "Iteration 12, loss = 0.37159913\n",
      "Iteration 13, loss = 0.37035722\n",
      "Iteration 14, loss = 0.36904549\n",
      "Iteration 15, loss = 0.36773751\n",
      "Iteration 16, loss = 0.36645189\n",
      "Iteration 17, loss = 0.36511945\n",
      "Iteration 18, loss = 0.36378687\n",
      "Iteration 19, loss = 0.36242470\n",
      "Iteration 20, loss = 0.36106101\n",
      "Iteration 21, loss = 0.35965282\n",
      "Iteration 22, loss = 0.35827465\n",
      "Iteration 23, loss = 0.35679963\n",
      "Iteration 24, loss = 0.35532080\n",
      "Iteration 25, loss = 0.35386112\n",
      "Iteration 26, loss = 0.35238922\n",
      "Iteration 27, loss = 0.35085646\n",
      "Iteration 28, loss = 0.34933694\n",
      "Iteration 29, loss = 0.34777337\n",
      "Iteration 30, loss = 0.34628352\n",
      "Iteration 31, loss = 0.34462788\n",
      "Iteration 32, loss = 0.34304542\n",
      "Iteration 33, loss = 0.34143631\n",
      "Iteration 34, loss = 0.33981880\n",
      "Iteration 35, loss = 0.33825765\n",
      "Iteration 36, loss = 0.33655013\n",
      "Iteration 37, loss = 0.33492623\n",
      "Iteration 38, loss = 0.33327977\n",
      "Iteration 39, loss = 0.33157311\n",
      "Iteration 40, loss = 0.32987806\n",
      "Iteration 41, loss = 0.32824137\n",
      "Iteration 42, loss = 0.32652844\n",
      "Iteration 43, loss = 0.32482818\n",
      "Iteration 44, loss = 0.32319523\n",
      "Iteration 45, loss = 0.32146504\n",
      "Iteration 46, loss = 0.31972263\n",
      "Iteration 47, loss = 0.31804957\n",
      "Iteration 48, loss = 0.31635738\n",
      "Iteration 49, loss = 0.31464130\n",
      "Iteration 50, loss = 0.31299203\n",
      "Iteration 51, loss = 0.31128664\n",
      "Iteration 52, loss = 0.30956967\n",
      "Iteration 53, loss = 0.30789245\n",
      "Iteration 54, loss = 0.30627696\n",
      "Iteration 55, loss = 0.30452034\n",
      "Iteration 56, loss = 0.30284606\n",
      "Iteration 57, loss = 0.30115112\n",
      "Iteration 58, loss = 0.29941846\n",
      "Iteration 59, loss = 0.29775045\n",
      "Iteration 60, loss = 0.29599736\n",
      "Iteration 61, loss = 0.29429225\n",
      "Iteration 62, loss = 0.29258507\n",
      "Iteration 63, loss = 0.29089368\n",
      "Iteration 64, loss = 0.28916907\n",
      "Iteration 65, loss = 0.28749201\n",
      "Iteration 66, loss = 0.28575236\n",
      "Iteration 67, loss = 0.28405059\n",
      "Iteration 68, loss = 0.28234459\n",
      "Iteration 69, loss = 0.28060927\n",
      "Iteration 70, loss = 0.27892069\n",
      "Iteration 71, loss = 0.27715789\n",
      "Iteration 72, loss = 0.27540378\n",
      "Iteration 73, loss = 0.27363832\n",
      "Iteration 74, loss = 0.27187361\n",
      "Iteration 75, loss = 0.27006599\n",
      "Iteration 76, loss = 0.26822563\n",
      "Iteration 77, loss = 0.26645025\n",
      "Iteration 78, loss = 0.26462052\n",
      "Iteration 79, loss = 0.26278790\n",
      "Iteration 80, loss = 0.26097993\n",
      "Iteration 81, loss = 0.25914975\n",
      "Iteration 82, loss = 0.25730624\n",
      "Iteration 83, loss = 0.25554490\n",
      "Iteration 84, loss = 0.25366658\n",
      "Iteration 85, loss = 0.25183394\n",
      "Iteration 86, loss = 0.24999925\n",
      "Iteration 87, loss = 0.24816206\n",
      "Iteration 88, loss = 0.24633912\n",
      "Iteration 89, loss = 0.24459865\n",
      "Iteration 90, loss = 0.24274920\n",
      "Iteration 91, loss = 0.24099960\n",
      "Iteration 92, loss = 0.23924368\n",
      "Iteration 93, loss = 0.23744907\n",
      "Iteration 94, loss = 0.23567264\n",
      "Iteration 95, loss = 0.23392888\n",
      "Iteration 96, loss = 0.23216495\n",
      "Iteration 97, loss = 0.23035987\n",
      "Iteration 98, loss = 0.22861457\n",
      "Iteration 99, loss = 0.22687641\n",
      "Iteration 100, loss = 0.22507972\n",
      "Iteration 101, loss = 0.22333773\n",
      "Iteration 102, loss = 0.22161420\n",
      "Iteration 103, loss = 0.21989700\n",
      "Iteration 104, loss = 0.21822089\n",
      "Iteration 105, loss = 0.21662889\n",
      "Iteration 106, loss = 0.21502077\n",
      "Iteration 107, loss = 0.21347623\n",
      "Iteration 108, loss = 0.21188204\n",
      "Iteration 109, loss = 0.21033152\n",
      "Iteration 110, loss = 0.20879916\n",
      "Iteration 111, loss = 0.20723802\n",
      "Iteration 112, loss = 0.20570943\n",
      "Iteration 113, loss = 0.20416998\n",
      "Iteration 114, loss = 0.20265475\n",
      "Iteration 115, loss = 0.20116013\n",
      "Iteration 116, loss = 0.19975123\n",
      "Iteration 117, loss = 0.19835074\n",
      "Iteration 118, loss = 0.19694942\n",
      "Iteration 119, loss = 0.19559615\n",
      "Iteration 120, loss = 0.19421681\n",
      "Iteration 121, loss = 0.19295160\n",
      "Iteration 122, loss = 0.19161787\n",
      "Iteration 123, loss = 0.19026748\n",
      "Iteration 124, loss = 0.18900138\n",
      "Iteration 125, loss = 0.18769188\n",
      "Iteration 126, loss = 0.18633769\n",
      "Iteration 127, loss = 0.18505307\n",
      "Iteration 128, loss = 0.18378116\n",
      "Iteration 129, loss = 0.18251001\n",
      "Iteration 130, loss = 0.18122157\n",
      "Iteration 131, loss = 0.17996433\n",
      "Iteration 132, loss = 0.17878871\n",
      "Iteration 133, loss = 0.17758018\n",
      "Iteration 134, loss = 0.17643345\n",
      "Iteration 135, loss = 0.17523341\n",
      "Iteration 136, loss = 0.17408934\n",
      "Iteration 137, loss = 0.17296055\n",
      "Iteration 138, loss = 0.17191035\n",
      "Iteration 139, loss = 0.17085601\n",
      "Iteration 140, loss = 0.16979514\n",
      "Iteration 141, loss = 0.16878103\n",
      "Iteration 142, loss = 0.16778858\n",
      "Iteration 143, loss = 0.16678565\n",
      "Iteration 144, loss = 0.16584824\n",
      "Iteration 145, loss = 0.16487049\n",
      "Iteration 146, loss = 0.16398494\n",
      "Iteration 147, loss = 0.16301619\n",
      "Iteration 148, loss = 0.16217257\n",
      "Iteration 149, loss = 0.16134791\n",
      "Iteration 150, loss = 0.16045130\n",
      "Iteration 151, loss = 0.15958439\n",
      "Iteration 152, loss = 0.15871548\n",
      "Iteration 153, loss = 0.15791317\n",
      "Iteration 154, loss = 0.15710854\n",
      "Iteration 155, loss = 0.15632839\n",
      "Iteration 156, loss = 0.15552622\n",
      "Iteration 157, loss = 0.15479218\n",
      "Iteration 158, loss = 0.15401395\n",
      "Iteration 159, loss = 0.15327403\n",
      "Iteration 160, loss = 0.15254269\n",
      "Iteration 161, loss = 0.15184642\n",
      "Iteration 162, loss = 0.15115314\n",
      "Iteration 163, loss = 0.15051142\n",
      "Iteration 164, loss = 0.14980926\n",
      "Iteration 165, loss = 0.14919804\n",
      "Iteration 166, loss = 0.14851179\n",
      "Iteration 167, loss = 0.14787228\n",
      "Iteration 168, loss = 0.14724342\n",
      "Iteration 169, loss = 0.14662704\n",
      "Iteration 170, loss = 0.14603883\n",
      "Iteration 171, loss = 0.14542141\n",
      "Iteration 172, loss = 0.14484337\n",
      "Iteration 173, loss = 0.14424204\n",
      "Iteration 174, loss = 0.14369819\n",
      "Iteration 175, loss = 0.14311496\n",
      "Iteration 176, loss = 0.14256378\n",
      "Iteration 177, loss = 0.14205725\n",
      "Iteration 178, loss = 0.14154498\n",
      "Iteration 179, loss = 0.14098778\n",
      "Iteration 180, loss = 0.14047276\n",
      "Iteration 181, loss = 0.13996240\n",
      "Iteration 182, loss = 0.13949337\n",
      "Iteration 183, loss = 0.13898859\n",
      "Iteration 184, loss = 0.13852171\n",
      "Iteration 185, loss = 0.13811351\n",
      "Iteration 186, loss = 0.13758947\n",
      "Iteration 187, loss = 0.13714625\n",
      "Iteration 188, loss = 0.13669131\n",
      "Iteration 189, loss = 0.13624074\n",
      "Iteration 190, loss = 0.13587125\n",
      "Iteration 191, loss = 0.13555689\n",
      "Iteration 192, loss = 0.13530058\n",
      "Iteration 193, loss = 0.13502934\n",
      "Iteration 194, loss = 0.13485358\n",
      "Iteration 195, loss = 0.13463580\n",
      "Iteration 196, loss = 0.13432637\n",
      "Iteration 197, loss = 0.13409111\n",
      "Iteration 198, loss = 0.13390014\n",
      "Iteration 199, loss = 0.13369247\n",
      "Iteration 200, loss = 0.13349743\n",
      "Iteration 201, loss = 0.13331448\n",
      "Iteration 202, loss = 0.13313363\n",
      "Iteration 203, loss = 0.13291611\n",
      "Iteration 204, loss = 0.13273308\n",
      "Iteration 205, loss = 0.13253347\n",
      "Iteration 206, loss = 0.13234851\n",
      "Iteration 207, loss = 0.13216615\n",
      "Iteration 208, loss = 0.13202325\n",
      "Iteration 209, loss = 0.13182307\n",
      "Iteration 210, loss = 0.13166164\n",
      "Iteration 211, loss = 0.13151075\n",
      "Iteration 212, loss = 0.13136250\n",
      "Iteration 213, loss = 0.13118436\n",
      "Iteration 214, loss = 0.13106577\n",
      "Iteration 215, loss = 0.13087411\n",
      "Iteration 216, loss = 0.13070209\n",
      "Iteration 217, loss = 0.13056421\n",
      "Iteration 218, loss = 0.13042432\n",
      "Iteration 219, loss = 0.13029352\n",
      "Iteration 220, loss = 0.13016530\n",
      "Iteration 221, loss = 0.13002391\n",
      "Iteration 222, loss = 0.12991998\n",
      "Iteration 223, loss = 0.12979178\n",
      "Iteration 224, loss = 0.12963578\n",
      "Iteration 225, loss = 0.12952619\n",
      "Iteration 226, loss = 0.12942259\n",
      "Iteration 227, loss = 0.12928230\n",
      "Iteration 228, loss = 0.12918680\n",
      "Iteration 229, loss = 0.12904173\n",
      "Iteration 230, loss = 0.12892400\n",
      "Iteration 231, loss = 0.12881249\n",
      "Iteration 232, loss = 0.12868075\n",
      "Iteration 233, loss = 0.12854393\n",
      "Iteration 234, loss = 0.12845229\n",
      "Iteration 235, loss = 0.12833603\n",
      "Iteration 236, loss = 0.12829653\n",
      "Iteration 237, loss = 0.12814148\n",
      "Iteration 238, loss = 0.12802960\n",
      "Iteration 239, loss = 0.12794827\n",
      "Iteration 240, loss = 0.12782310\n",
      "Iteration 241, loss = 0.12778262\n",
      "Iteration 242, loss = 0.12763140\n",
      "Iteration 243, loss = 0.12758801\n",
      "Iteration 244, loss = 0.12749291\n",
      "Iteration 245, loss = 0.12740238\n",
      "Iteration 246, loss = 0.12733125\n",
      "Iteration 247, loss = 0.12728327\n",
      "Iteration 248, loss = 0.12710957\n",
      "Iteration 249, loss = 0.12701152\n",
      "Iteration 250, loss = 0.12697432\n",
      "Iteration 251, loss = 0.12687160\n",
      "Iteration 252, loss = 0.12675421\n",
      "Iteration 253, loss = 0.12669992\n",
      "Iteration 254, loss = 0.12660662\n",
      "Iteration 255, loss = 0.12652873\n",
      "Iteration 256, loss = 0.12645186\n",
      "Iteration 257, loss = 0.12642733\n",
      "Iteration 258, loss = 0.12630045\n",
      "Iteration 259, loss = 0.12621844\n",
      "Iteration 260, loss = 0.12614097\n",
      "Iteration 261, loss = 0.12607357\n",
      "Iteration 262, loss = 0.12600257\n",
      "Iteration 263, loss = 0.12595463\n",
      "Iteration 264, loss = 0.12586750\n",
      "Iteration 265, loss = 0.12580067\n",
      "Iteration 266, loss = 0.12573849\n",
      "Iteration 267, loss = 0.12566594\n",
      "Iteration 268, loss = 0.12561117\n",
      "Iteration 269, loss = 0.12550646\n",
      "Iteration 270, loss = 0.12548437\n",
      "Iteration 271, loss = 0.12537456\n",
      "Iteration 272, loss = 0.12535818\n",
      "Iteration 273, loss = 0.12528145\n",
      "Iteration 274, loss = 0.12519212\n",
      "Iteration 275, loss = 0.12514367\n",
      "Iteration 276, loss = 0.12514345\n",
      "Iteration 277, loss = 0.12498686\n",
      "Iteration 278, loss = 0.12494013\n",
      "Iteration 279, loss = 0.12494248\n",
      "Iteration 280, loss = 0.12486732\n",
      "Iteration 281, loss = 0.12482669\n",
      "Iteration 282, loss = 0.12478334\n",
      "Iteration 283, loss = 0.12468228\n",
      "Iteration 284, loss = 0.12462954\n",
      "Iteration 285, loss = 0.12456352\n",
      "Iteration 286, loss = 0.12452429\n",
      "Iteration 287, loss = 0.12446362\n",
      "Iteration 288, loss = 0.12443606\n",
      "Iteration 289, loss = 0.12435873\n",
      "Iteration 290, loss = 0.12430051\n",
      "Iteration 291, loss = 0.12424832\n",
      "Iteration 292, loss = 0.12422942\n",
      "Iteration 293, loss = 0.12413916\n",
      "Iteration 294, loss = 0.12409914\n",
      "Iteration 295, loss = 0.12404315\n",
      "Iteration 296, loss = 0.12400152\n",
      "Iteration 297, loss = 0.12393167\n",
      "Iteration 298, loss = 0.12388716\n",
      "Iteration 299, loss = 0.12383199\n",
      "Iteration 300, loss = 0.12383112\n",
      "Iteration 301, loss = 0.12375410\n",
      "Iteration 302, loss = 0.12376060\n",
      "Iteration 303, loss = 0.12372527\n",
      "Iteration 304, loss = 0.12364817\n",
      "Iteration 305, loss = 0.12361008\n",
      "Iteration 306, loss = 0.12352462\n",
      "Iteration 307, loss = 0.12348442\n",
      "Iteration 308, loss = 0.12344282\n",
      "Iteration 309, loss = 0.12340625\n",
      "Iteration 310, loss = 0.12336646\n",
      "Iteration 311, loss = 0.12330585\n",
      "Iteration 312, loss = 0.12325437\n",
      "Iteration 313, loss = 0.12325235\n",
      "Iteration 314, loss = 0.12315178\n",
      "Iteration 315, loss = 0.12312782\n",
      "Iteration 316, loss = 0.12310622\n",
      "Iteration 317, loss = 0.12308167\n",
      "Iteration 318, loss = 0.12303283\n",
      "Iteration 319, loss = 0.12298649\n",
      "Iteration 320, loss = 0.12295975\n",
      "Iteration 321, loss = 0.12294970\n",
      "Iteration 322, loss = 0.12285642\n",
      "Iteration 323, loss = 0.12284577\n",
      "Iteration 324, loss = 0.12283027\n",
      "Iteration 325, loss = 0.12277004\n",
      "Iteration 326, loss = 0.12269991\n",
      "Iteration 327, loss = 0.12266117\n",
      "Iteration 328, loss = 0.12269575\n",
      "Iteration 329, loss = 0.12258832\n",
      "Iteration 330, loss = 0.12257078\n",
      "Iteration 331, loss = 0.12250996\n",
      "Iteration 332, loss = 0.12254307\n",
      "Iteration 333, loss = 0.12249105\n",
      "Iteration 334, loss = 0.12243728\n",
      "Iteration 335, loss = 0.12240287\n",
      "Iteration 336, loss = 0.12238252\n",
      "Iteration 337, loss = 0.12234091\n",
      "Iteration 338, loss = 0.12229489\n",
      "Iteration 339, loss = 0.12225851\n",
      "Iteration 340, loss = 0.12223822\n",
      "Iteration 341, loss = 0.12220212\n",
      "Iteration 342, loss = 0.12216731\n",
      "Iteration 343, loss = 0.12218749\n",
      "Iteration 344, loss = 0.12207540\n",
      "Iteration 345, loss = 0.12206507\n",
      "Iteration 346, loss = 0.12201710\n",
      "Iteration 347, loss = 0.12199309\n",
      "Iteration 348, loss = 0.12196617\n",
      "Iteration 349, loss = 0.12192565\n",
      "Iteration 350, loss = 0.12192771\n",
      "Iteration 351, loss = 0.12186039\n",
      "Iteration 352, loss = 0.12185461\n",
      "Iteration 353, loss = 0.12180469\n",
      "Iteration 354, loss = 0.12179134\n",
      "Iteration 355, loss = 0.12175094\n",
      "Iteration 356, loss = 0.12173982\n",
      "Iteration 357, loss = 0.12169628\n",
      "Iteration 358, loss = 0.12166619\n",
      "Iteration 359, loss = 0.12163158\n",
      "Iteration 360, loss = 0.12160632\n",
      "Iteration 361, loss = 0.12162041\n",
      "Iteration 362, loss = 0.12153891\n",
      "Iteration 363, loss = 0.12152754\n",
      "Iteration 364, loss = 0.12158659\n",
      "Iteration 365, loss = 0.12148360\n",
      "Iteration 366, loss = 0.12147866\n",
      "Iteration 367, loss = 0.12143816\n",
      "Iteration 368, loss = 0.12139240\n",
      "Iteration 369, loss = 0.12139229\n",
      "Iteration 370, loss = 0.12136600\n",
      "Iteration 371, loss = 0.12130999\n",
      "Iteration 372, loss = 0.12133844\n",
      "Iteration 373, loss = 0.12125646\n",
      "Iteration 374, loss = 0.12123749\n",
      "Iteration 375, loss = 0.12121638\n",
      "Iteration 376, loss = 0.12120357\n",
      "Iteration 377, loss = 0.12118378\n",
      "Iteration 378, loss = 0.12112997\n",
      "Iteration 379, loss = 0.12111548\n",
      "Iteration 380, loss = 0.12108059\n",
      "Iteration 381, loss = 0.12105537\n",
      "Iteration 382, loss = 0.12103495\n",
      "Iteration 383, loss = 0.12101071\n",
      "Iteration 384, loss = 0.12103723\n",
      "Iteration 385, loss = 0.12096895\n",
      "Iteration 386, loss = 0.12094975\n",
      "Iteration 387, loss = 0.12092790\n",
      "Iteration 388, loss = 0.12091094\n",
      "Iteration 389, loss = 0.12085907\n",
      "Iteration 390, loss = 0.12093898\n",
      "Iteration 391, loss = 0.12079883\n",
      "Iteration 392, loss = 0.12080352\n",
      "Iteration 393, loss = 0.12081425\n",
      "Iteration 394, loss = 0.12078040\n",
      "Iteration 395, loss = 0.12074471\n",
      "Iteration 396, loss = 0.12070980\n",
      "Iteration 397, loss = 0.12068714\n",
      "Iteration 398, loss = 0.12068873\n",
      "Iteration 399, loss = 0.12066912\n",
      "Iteration 400, loss = 0.12061888\n",
      "Iteration 401, loss = 0.12054797\n",
      "Iteration 402, loss = 0.12055920\n",
      "Iteration 403, loss = 0.12055390\n",
      "Iteration 404, loss = 0.12050811\n",
      "Iteration 405, loss = 0.12051103\n",
      "Iteration 406, loss = 0.12046102\n",
      "Iteration 407, loss = 0.12043465\n",
      "Iteration 408, loss = 0.12046281\n",
      "Iteration 409, loss = 0.12050763\n",
      "Iteration 410, loss = 0.12039372\n",
      "Iteration 411, loss = 0.12037780\n",
      "Iteration 412, loss = 0.12038650\n",
      "Iteration 413, loss = 0.12033804\n",
      "Iteration 414, loss = 0.12030168\n",
      "Iteration 415, loss = 0.12026906\n",
      "Iteration 416, loss = 0.12033279\n",
      "Iteration 417, loss = 0.12025797\n",
      "Iteration 418, loss = 0.12020774\n",
      "Iteration 419, loss = 0.12023535\n",
      "Iteration 420, loss = 0.12017428\n",
      "Iteration 421, loss = 0.12017020\n",
      "Iteration 422, loss = 0.12013715\n",
      "Iteration 423, loss = 0.12012731\n",
      "Iteration 424, loss = 0.12009943\n",
      "Iteration 425, loss = 0.12009754\n",
      "Iteration 426, loss = 0.12008052\n",
      "Iteration 427, loss = 0.12003979\n",
      "Iteration 428, loss = 0.12003122\n",
      "Iteration 429, loss = 0.11999555\n",
      "Iteration 430, loss = 0.11998287\n",
      "Iteration 431, loss = 0.11998010\n",
      "Iteration 432, loss = 0.11995778\n",
      "Iteration 433, loss = 0.11990416\n",
      "Iteration 434, loss = 0.11992724\n",
      "Iteration 435, loss = 0.11988978\n",
      "Iteration 436, loss = 0.11984313\n",
      "Iteration 437, loss = 0.11987722\n",
      "Iteration 438, loss = 0.11984702\n",
      "Iteration 439, loss = 0.11981767\n",
      "Iteration 440, loss = 0.11979444\n",
      "Iteration 441, loss = 0.11977457\n",
      "Iteration 442, loss = 0.11976591\n",
      "Iteration 443, loss = 0.11973211\n",
      "Iteration 444, loss = 0.11975791\n",
      "Iteration 445, loss = 0.11975877\n",
      "Iteration 446, loss = 0.11973344\n",
      "Iteration 447, loss = 0.11971673\n",
      "Iteration 448, loss = 0.11964643\n",
      "Iteration 449, loss = 0.11966102\n",
      "Iteration 450, loss = 0.11969621\n",
      "Iteration 451, loss = 0.11961496\n",
      "Iteration 452, loss = 0.11959257\n",
      "Iteration 453, loss = 0.11959144\n",
      "Iteration 454, loss = 0.11961184\n",
      "Iteration 455, loss = 0.11953353\n",
      "Iteration 456, loss = 0.11954268\n",
      "Iteration 457, loss = 0.11952815\n",
      "Iteration 458, loss = 0.11953151\n",
      "Iteration 459, loss = 0.11948759\n",
      "Iteration 460, loss = 0.11945773\n",
      "Iteration 461, loss = 0.11944069\n",
      "Iteration 462, loss = 0.11942523\n",
      "Iteration 463, loss = 0.11944561\n",
      "Iteration 464, loss = 0.11947230\n",
      "Iteration 465, loss = 0.11941451\n",
      "Iteration 466, loss = 0.11937544\n",
      "Iteration 467, loss = 0.11938033\n",
      "Iteration 468, loss = 0.11932062\n",
      "Iteration 469, loss = 0.11931405\n",
      "Iteration 470, loss = 0.11928731\n",
      "Iteration 471, loss = 0.11929182\n",
      "Iteration 472, loss = 0.11925971\n",
      "Iteration 473, loss = 0.11923733\n",
      "Iteration 474, loss = 0.11930406\n",
      "Iteration 475, loss = 0.11920408\n",
      "Iteration 476, loss = 0.11921862\n",
      "Iteration 477, loss = 0.11921585\n",
      "Iteration 478, loss = 0.11921063\n",
      "Iteration 479, loss = 0.11915676\n",
      "Iteration 480, loss = 0.11917031\n",
      "Iteration 481, loss = 0.11916334\n",
      "Iteration 482, loss = 0.11913003\n",
      "Iteration 483, loss = 0.11912106\n",
      "Iteration 484, loss = 0.11909732\n",
      "Iteration 485, loss = 0.11910901\n",
      "Iteration 486, loss = 0.11906496\n",
      "Iteration 487, loss = 0.11908376\n",
      "Iteration 488, loss = 0.11902660\n",
      "Iteration 489, loss = 0.11916274\n",
      "Iteration 490, loss = 0.11906761\n",
      "Iteration 491, loss = 0.11899611\n",
      "Iteration 492, loss = 0.11900601\n",
      "Iteration 493, loss = 0.11899263\n",
      "Iteration 494, loss = 0.11895139\n",
      "Iteration 495, loss = 0.11898566\n",
      "Iteration 496, loss = 0.11893138\n",
      "Iteration 497, loss = 0.11897986\n",
      "Iteration 498, loss = 0.11888626\n",
      "Iteration 499, loss = 0.11898451\n",
      "Iteration 500, loss = 0.11889814\n",
      "Iteration 501, loss = 0.11886215\n",
      "Iteration 502, loss = 0.11889794\n",
      "Iteration 503, loss = 0.11884201\n",
      "Iteration 504, loss = 0.11881298\n",
      "Iteration 505, loss = 0.11881071\n",
      "Iteration 506, loss = 0.11880397\n",
      "Iteration 507, loss = 0.11877291\n",
      "Iteration 508, loss = 0.11877481\n",
      "Iteration 509, loss = 0.11875715\n",
      "Iteration 510, loss = 0.11873081\n",
      "Iteration 511, loss = 0.11870124\n",
      "Iteration 512, loss = 0.11869413\n",
      "Iteration 513, loss = 0.11870246\n",
      "Iteration 514, loss = 0.11867608\n",
      "Iteration 515, loss = 0.11865660\n",
      "Iteration 516, loss = 0.11866742\n",
      "Iteration 517, loss = 0.11878963\n",
      "Iteration 518, loss = 0.11861974\n",
      "Iteration 519, loss = 0.11863086\n",
      "Iteration 520, loss = 0.11857827\n",
      "Iteration 521, loss = 0.11857409\n",
      "Iteration 522, loss = 0.11854801\n",
      "Iteration 523, loss = 0.11855801\n",
      "Iteration 524, loss = 0.11851786\n",
      "Iteration 525, loss = 0.11852999\n",
      "Iteration 526, loss = 0.11850483\n",
      "Iteration 527, loss = 0.11848522\n",
      "Iteration 528, loss = 0.11848518\n",
      "Iteration 529, loss = 0.11847948\n",
      "Iteration 530, loss = 0.11843641\n",
      "Iteration 531, loss = 0.11843174\n",
      "Iteration 532, loss = 0.11841421\n",
      "Iteration 533, loss = 0.11838591\n",
      "Iteration 534, loss = 0.11838031\n",
      "Iteration 535, loss = 0.11832277\n",
      "Iteration 536, loss = 0.11830695\n",
      "Iteration 537, loss = 0.11837048\n",
      "Iteration 538, loss = 0.11825321\n",
      "Iteration 539, loss = 0.11823254\n",
      "Iteration 540, loss = 0.11820275\n",
      "Iteration 541, loss = 0.11815093\n",
      "Iteration 542, loss = 0.11821976\n",
      "Iteration 543, loss = 0.11813508\n",
      "Iteration 544, loss = 0.11816056\n",
      "Iteration 545, loss = 0.11807697\n",
      "Iteration 546, loss = 0.11804299\n",
      "Iteration 547, loss = 0.11808670\n",
      "Iteration 548, loss = 0.11802430\n",
      "Iteration 549, loss = 0.11801773\n",
      "Iteration 550, loss = 0.11800909\n",
      "Iteration 551, loss = 0.11801169\n",
      "Iteration 552, loss = 0.11796935\n",
      "Iteration 553, loss = 0.11797034\n",
      "Iteration 554, loss = 0.11798974\n",
      "Iteration 555, loss = 0.11792633\n",
      "Iteration 556, loss = 0.11806146\n",
      "Iteration 557, loss = 0.11789283\n",
      "Iteration 558, loss = 0.11786558\n",
      "Iteration 559, loss = 0.11790073\n",
      "Iteration 560, loss = 0.11781640\n",
      "Iteration 561, loss = 0.11779454\n",
      "Iteration 562, loss = 0.11778269\n",
      "Iteration 563, loss = 0.11777371\n",
      "Iteration 564, loss = 0.11777907\n",
      "Iteration 565, loss = 0.11777379\n",
      "Iteration 566, loss = 0.11775878\n",
      "Iteration 567, loss = 0.11772399\n",
      "Iteration 568, loss = 0.11770657\n",
      "Iteration 569, loss = 0.11768910\n",
      "Iteration 570, loss = 0.11770166\n",
      "Iteration 571, loss = 0.11767427\n",
      "Iteration 572, loss = 0.11766904\n",
      "Iteration 573, loss = 0.11763593\n",
      "Iteration 574, loss = 0.11761982\n",
      "Iteration 575, loss = 0.11762198\n",
      "Iteration 576, loss = 0.11759974\n",
      "Iteration 577, loss = 0.11758324\n",
      "Iteration 578, loss = 0.11755277\n",
      "Iteration 579, loss = 0.11754982\n",
      "Iteration 580, loss = 0.11751080\n",
      "Iteration 581, loss = 0.11752607\n",
      "Iteration 582, loss = 0.11748174\n",
      "Iteration 583, loss = 0.11748225\n",
      "Iteration 584, loss = 0.11746734\n",
      "Iteration 585, loss = 0.11747799\n",
      "Iteration 586, loss = 0.11745953\n",
      "Iteration 587, loss = 0.11744718\n",
      "Iteration 588, loss = 0.11743150\n",
      "Iteration 589, loss = 0.11738657\n",
      "Iteration 590, loss = 0.11737535\n",
      "Iteration 591, loss = 0.11737142\n",
      "Iteration 592, loss = 0.11739819\n",
      "Iteration 593, loss = 0.11738288\n",
      "Iteration 594, loss = 0.11738187\n",
      "Iteration 595, loss = 0.11732236\n",
      "Iteration 596, loss = 0.11731025\n",
      "Iteration 597, loss = 0.11727990\n",
      "Iteration 598, loss = 0.11727932\n",
      "Iteration 599, loss = 0.11734439\n",
      "Iteration 600, loss = 0.11724855\n",
      "Iteration 601, loss = 0.11724417\n",
      "Iteration 602, loss = 0.11724625\n",
      "Iteration 603, loss = 0.11725472\n",
      "Iteration 604, loss = 0.11723139\n",
      "Iteration 605, loss = 0.11721195\n",
      "Iteration 606, loss = 0.11719715\n",
      "Iteration 607, loss = 0.11720437\n",
      "Iteration 608, loss = 0.11722739\n",
      "Iteration 609, loss = 0.11712888\n",
      "Iteration 610, loss = 0.11715001\n",
      "Iteration 611, loss = 0.11712257\n",
      "Iteration 612, loss = 0.11711291\n",
      "Iteration 613, loss = 0.11709225\n",
      "Iteration 614, loss = 0.11707648\n",
      "Iteration 615, loss = 0.11709805\n",
      "Iteration 616, loss = 0.11710947\n",
      "Iteration 617, loss = 0.11711446\n",
      "Iteration 618, loss = 0.11703954\n",
      "Iteration 619, loss = 0.11704897\n",
      "Iteration 620, loss = 0.11703399\n",
      "Iteration 621, loss = 0.11707829\n",
      "Iteration 622, loss = 0.11709063\n",
      "Iteration 623, loss = 0.11698420\n",
      "Iteration 624, loss = 0.11699318\n",
      "Iteration 625, loss = 0.11697137\n",
      "Iteration 626, loss = 0.11693352\n",
      "Iteration 627, loss = 0.11694337\n",
      "Iteration 628, loss = 0.11694474\n",
      "Iteration 629, loss = 0.11692634\n",
      "Iteration 630, loss = 0.11696713\n",
      "Iteration 631, loss = 0.11690614\n",
      "Iteration 632, loss = 0.11692098\n",
      "Iteration 633, loss = 0.11686189\n",
      "Iteration 634, loss = 0.11686351\n",
      "Iteration 635, loss = 0.11684897\n",
      "Iteration 636, loss = 0.11685525\n",
      "Iteration 637, loss = 0.11681934\n",
      "Iteration 638, loss = 0.11682370\n",
      "Iteration 639, loss = 0.11682382\n",
      "Iteration 640, loss = 0.11680173\n",
      "Iteration 641, loss = 0.11678697\n",
      "Iteration 642, loss = 0.11676473\n",
      "Iteration 643, loss = 0.11676342\n",
      "Iteration 644, loss = 0.11677287\n",
      "Iteration 645, loss = 0.11683465\n",
      "Iteration 646, loss = 0.11673578\n",
      "Iteration 647, loss = 0.11674727\n",
      "Iteration 648, loss = 0.11671098\n",
      "Iteration 649, loss = 0.11669698\n",
      "Iteration 650, loss = 0.11668545\n",
      "Iteration 651, loss = 0.11668467\n",
      "Iteration 652, loss = 0.11671912\n",
      "Iteration 653, loss = 0.11667278\n",
      "Iteration 654, loss = 0.11666774\n",
      "Iteration 655, loss = 0.11665655\n",
      "Iteration 656, loss = 0.11663443\n",
      "Iteration 657, loss = 0.11662942\n",
      "Iteration 658, loss = 0.11659384\n",
      "Iteration 659, loss = 0.11664853\n",
      "Iteration 660, loss = 0.11656640\n",
      "Iteration 661, loss = 0.11657939\n",
      "Iteration 662, loss = 0.11660309\n",
      "Iteration 663, loss = 0.11659812\n",
      "Iteration 664, loss = 0.11659069\n",
      "Iteration 665, loss = 0.11659844\n",
      "Iteration 666, loss = 0.11655287\n",
      "Iteration 667, loss = 0.11653314\n",
      "Iteration 668, loss = 0.11654798\n",
      "Iteration 669, loss = 0.11654571\n",
      "Iteration 670, loss = 0.11650103\n",
      "Iteration 671, loss = 0.11653056\n",
      "Iteration 672, loss = 0.11654305\n",
      "Iteration 673, loss = 0.11652854\n",
      "Iteration 674, loss = 0.11649264\n",
      "Iteration 675, loss = 0.11647760\n",
      "Iteration 676, loss = 0.11643848\n",
      "Iteration 677, loss = 0.11646008\n",
      "Iteration 678, loss = 0.11642364\n",
      "Iteration 679, loss = 0.11649365\n",
      "Iteration 680, loss = 0.11645430\n",
      "Iteration 681, loss = 0.11642854\n",
      "Iteration 682, loss = 0.11644454\n",
      "Iteration 683, loss = 0.11641508\n",
      "Iteration 684, loss = 0.11642198\n",
      "Iteration 685, loss = 0.11637927\n",
      "Iteration 686, loss = 0.11639764\n",
      "Iteration 687, loss = 0.11635994\n",
      "Iteration 688, loss = 0.11636016\n",
      "Iteration 689, loss = 0.11644287\n",
      "Iteration 690, loss = 0.11638368\n",
      "Iteration 691, loss = 0.11635469\n",
      "Iteration 692, loss = 0.11633931\n",
      "Iteration 693, loss = 0.11632324\n",
      "Iteration 694, loss = 0.11636928\n",
      "Iteration 695, loss = 0.11636691\n",
      "Iteration 696, loss = 0.11637167\n",
      "Iteration 697, loss = 0.11631621\n",
      "Iteration 698, loss = 0.11628476\n",
      "Iteration 699, loss = 0.11628807\n",
      "Iteration 700, loss = 0.11625258\n",
      "Iteration 701, loss = 0.11627588\n",
      "Iteration 702, loss = 0.11624342\n",
      "Iteration 703, loss = 0.11627494\n",
      "Iteration 704, loss = 0.11622611\n",
      "Iteration 705, loss = 0.11625134\n",
      "Iteration 706, loss = 0.11620318\n",
      "Iteration 707, loss = 0.11619362\n",
      "Iteration 708, loss = 0.11619652\n",
      "Iteration 709, loss = 0.11618889\n",
      "Iteration 710, loss = 0.11619493\n",
      "Iteration 711, loss = 0.11618169\n",
      "Iteration 712, loss = 0.11616909\n",
      "Iteration 713, loss = 0.11619716\n",
      "Iteration 714, loss = 0.11615870\n",
      "Iteration 715, loss = 0.11615391\n",
      "Iteration 716, loss = 0.11613566\n",
      "Iteration 717, loss = 0.11613159\n",
      "Iteration 718, loss = 0.11610789\n",
      "Iteration 719, loss = 0.11616195\n",
      "Iteration 720, loss = 0.11611614\n",
      "Iteration 721, loss = 0.11617342\n",
      "Iteration 722, loss = 0.11608563\n",
      "Iteration 723, loss = 0.11614268\n",
      "Iteration 724, loss = 0.11607625\n",
      "Iteration 725, loss = 0.11607580\n",
      "Iteration 726, loss = 0.11609604\n",
      "Iteration 727, loss = 0.11610566\n",
      "Iteration 728, loss = 0.11606963\n",
      "Iteration 729, loss = 0.11611522\n",
      "Iteration 730, loss = 0.11604677\n",
      "Iteration 731, loss = 0.11606923\n",
      "Iteration 732, loss = 0.11602140\n",
      "Iteration 733, loss = 0.11606882\n",
      "Iteration 734, loss = 0.11604130\n",
      "Iteration 735, loss = 0.11600187\n",
      "Iteration 736, loss = 0.11600625\n",
      "Iteration 737, loss = 0.11600717\n",
      "Iteration 738, loss = 0.11599381\n",
      "Iteration 739, loss = 0.11598489\n",
      "Iteration 740, loss = 0.11597410\n",
      "Iteration 741, loss = 0.11598852\n",
      "Iteration 742, loss = 0.11596140\n",
      "Iteration 743, loss = 0.11604123\n",
      "Iteration 744, loss = 0.11608547\n",
      "Iteration 745, loss = 0.11597654\n",
      "Iteration 746, loss = 0.11600009\n",
      "Iteration 747, loss = 0.11592910\n",
      "Iteration 748, loss = 0.11594117\n",
      "Iteration 749, loss = 0.11593817\n",
      "Iteration 750, loss = 0.11593120\n",
      "Iteration 751, loss = 0.11592596\n",
      "Iteration 752, loss = 0.11589827\n",
      "Iteration 753, loss = 0.11587505\n",
      "Iteration 754, loss = 0.11587646\n",
      "Iteration 755, loss = 0.11588661\n",
      "Iteration 756, loss = 0.11587513\n",
      "Iteration 757, loss = 0.11585618\n",
      "Iteration 758, loss = 0.11590065\n",
      "Iteration 759, loss = 0.11589288\n",
      "Iteration 760, loss = 0.11589040\n",
      "Iteration 761, loss = 0.11586343\n",
      "Iteration 762, loss = 0.11582830\n",
      "Iteration 763, loss = 0.11584948\n",
      "Iteration 764, loss = 0.11590156\n",
      "Iteration 765, loss = 0.11584013\n",
      "Iteration 766, loss = 0.11581588\n",
      "Iteration 767, loss = 0.11582021\n",
      "Iteration 768, loss = 0.11580165\n",
      "Iteration 769, loss = 0.11584283\n",
      "Iteration 770, loss = 0.11580917\n",
      "Iteration 771, loss = 0.11579037\n",
      "Iteration 772, loss = 0.11577983\n",
      "Iteration 773, loss = 0.11579029\n",
      "Iteration 774, loss = 0.11575645\n",
      "Iteration 775, loss = 0.11581757\n",
      "Iteration 776, loss = 0.11578016\n",
      "Iteration 777, loss = 0.11579233\n",
      "Iteration 778, loss = 0.11576522\n",
      "Iteration 779, loss = 0.11580984\n",
      "Iteration 780, loss = 0.11578073\n",
      "Iteration 781, loss = 0.11575420\n",
      "Iteration 782, loss = 0.11580925\n",
      "Iteration 783, loss = 0.11572860\n",
      "Iteration 784, loss = 0.11573083\n",
      "Iteration 785, loss = 0.11576308\n",
      "Iteration 786, loss = 0.11571877\n",
      "Iteration 787, loss = 0.11571062\n",
      "Iteration 788, loss = 0.11577129\n",
      "Iteration 789, loss = 0.11576465\n",
      "Iteration 790, loss = 0.11576323\n",
      "Iteration 791, loss = 0.11567325\n",
      "Iteration 792, loss = 0.11570277\n",
      "Iteration 793, loss = 0.11565764\n",
      "Iteration 794, loss = 0.11574039\n",
      "Iteration 795, loss = 0.11568854\n",
      "Iteration 796, loss = 0.11572437\n",
      "Iteration 797, loss = 0.11565996\n",
      "Iteration 798, loss = 0.11563871\n",
      "Iteration 799, loss = 0.11567574\n",
      "Iteration 800, loss = 0.11570702\n",
      "Iteration 801, loss = 0.11563488\n",
      "Iteration 802, loss = 0.11563819\n",
      "Iteration 803, loss = 0.11560436\n",
      "Iteration 804, loss = 0.11563695\n",
      "Iteration 805, loss = 0.11559885\n",
      "Iteration 806, loss = 0.11558536\n",
      "Iteration 807, loss = 0.11559590\n",
      "Iteration 808, loss = 0.11576929\n",
      "Iteration 809, loss = 0.11563869\n",
      "Iteration 810, loss = 0.11558569\n",
      "Iteration 811, loss = 0.11559687\n",
      "Iteration 812, loss = 0.11559627\n",
      "Iteration 813, loss = 0.11559711\n",
      "Iteration 814, loss = 0.11559086\n",
      "Iteration 815, loss = 0.11558930\n",
      "Iteration 816, loss = 0.11555485\n",
      "Iteration 817, loss = 0.11556049\n",
      "Iteration 818, loss = 0.11555700\n",
      "Iteration 819, loss = 0.11555414\n",
      "Iteration 820, loss = 0.11556521\n",
      "Iteration 821, loss = 0.11553655\n",
      "Iteration 822, loss = 0.11551614\n",
      "Iteration 823, loss = 0.11552804\n",
      "Iteration 824, loss = 0.11555537\n",
      "Iteration 825, loss = 0.11550432\n",
      "Iteration 826, loss = 0.11550635\n",
      "Iteration 827, loss = 0.11551803\n",
      "Iteration 828, loss = 0.11550444\n",
      "Iteration 829, loss = 0.11550884\n",
      "Iteration 830, loss = 0.11553511\n",
      "Iteration 831, loss = 0.11558124\n",
      "Iteration 832, loss = 0.11549372\n",
      "Iteration 833, loss = 0.11547512\n",
      "Iteration 834, loss = 0.11553203\n",
      "Iteration 835, loss = 0.11550805\n",
      "Iteration 836, loss = 0.11547455\n",
      "Iteration 837, loss = 0.11548396\n",
      "Iteration 838, loss = 0.11545313\n",
      "Iteration 839, loss = 0.11545062\n",
      "Iteration 840, loss = 0.11545925\n",
      "Iteration 841, loss = 0.11551726\n",
      "Iteration 842, loss = 0.11549496\n",
      "Iteration 843, loss = 0.11543660\n",
      "Iteration 844, loss = 0.11541864\n",
      "Iteration 845, loss = 0.11541420\n",
      "Iteration 846, loss = 0.11544218\n",
      "Iteration 847, loss = 0.11543817\n",
      "Iteration 848, loss = 0.11540503\n",
      "Iteration 849, loss = 0.11539668\n",
      "Iteration 850, loss = 0.11538144\n",
      "Iteration 851, loss = 0.11540022\n",
      "Iteration 852, loss = 0.11539985\n",
      "Iteration 853, loss = 0.11539101\n",
      "Iteration 854, loss = 0.11538848\n",
      "Iteration 855, loss = 0.11537996\n",
      "Iteration 856, loss = 0.11537811\n",
      "Iteration 857, loss = 0.11543402\n",
      "Iteration 858, loss = 0.11538042\n",
      "Iteration 859, loss = 0.11536790\n",
      "Iteration 860, loss = 0.11536805\n",
      "Iteration 861, loss = 0.11540525\n",
      "Iteration 862, loss = 0.11533482\n",
      "Iteration 863, loss = 0.11537931\n",
      "Iteration 864, loss = 0.11536031\n",
      "Iteration 865, loss = 0.11539571\n",
      "Iteration 866, loss = 0.11533810\n",
      "Iteration 867, loss = 0.11531484\n",
      "Iteration 868, loss = 0.11534816\n",
      "Iteration 869, loss = 0.11532170\n",
      "Iteration 870, loss = 0.11536826\n",
      "Iteration 871, loss = 0.11531651\n",
      "Iteration 872, loss = 0.11533272\n",
      "Iteration 873, loss = 0.11530587\n",
      "Iteration 874, loss = 0.11530829\n",
      "Iteration 875, loss = 0.11529189\n",
      "Iteration 876, loss = 0.11531092\n",
      "Iteration 877, loss = 0.11531557\n",
      "Iteration 878, loss = 0.11529351\n",
      "Iteration 879, loss = 0.11527316\n",
      "Iteration 880, loss = 0.11530444\n",
      "Iteration 881, loss = 0.11526890\n",
      "Iteration 882, loss = 0.11528930\n",
      "Iteration 883, loss = 0.11528381\n",
      "Iteration 884, loss = 0.11528883\n",
      "Iteration 885, loss = 0.11534455\n",
      "Iteration 886, loss = 0.11526539\n",
      "Iteration 887, loss = 0.11541340\n",
      "Iteration 888, loss = 0.11525385\n",
      "Iteration 889, loss = 0.11531745\n",
      "Iteration 890, loss = 0.11528069\n",
      "Iteration 891, loss = 0.11526333\n",
      "Iteration 892, loss = 0.11524473\n",
      "Iteration 893, loss = 0.11526358\n",
      "Iteration 894, loss = 0.11525073\n",
      "Iteration 895, loss = 0.11522622\n",
      "Iteration 896, loss = 0.11525847\n",
      "Iteration 897, loss = 0.11521687\n",
      "Iteration 898, loss = 0.11522120\n",
      "Iteration 899, loss = 0.11524728\n",
      "Iteration 900, loss = 0.11518520\n",
      "Iteration 901, loss = 0.11520175\n",
      "Iteration 902, loss = 0.11521133\n",
      "Iteration 903, loss = 0.11520233\n",
      "Iteration 904, loss = 0.11519243\n",
      "Iteration 905, loss = 0.11523767\n",
      "Iteration 906, loss = 0.11517430\n",
      "Iteration 907, loss = 0.11519450\n",
      "Iteration 908, loss = 0.11519065\n",
      "Iteration 909, loss = 0.11518137\n",
      "Iteration 910, loss = 0.11518382\n",
      "Iteration 911, loss = 0.11516930\n",
      "Iteration 912, loss = 0.11517498\n",
      "Iteration 913, loss = 0.11516422\n",
      "Iteration 914, loss = 0.11515426\n",
      "Iteration 915, loss = 0.11514231\n",
      "Iteration 916, loss = 0.11516909\n",
      "Iteration 917, loss = 0.11514478\n",
      "Iteration 918, loss = 0.11514615\n",
      "Iteration 919, loss = 0.11517113\n",
      "Iteration 920, loss = 0.11513765\n",
      "Iteration 921, loss = 0.11519021\n",
      "Iteration 922, loss = 0.11518199\n",
      "Iteration 923, loss = 0.11518774\n",
      "Iteration 924, loss = 0.11516493\n",
      "Iteration 925, loss = 0.11512816\n",
      "Iteration 926, loss = 0.11512752\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, tol=1e-05, verbose=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rede_neural_credit = MLPClassifier(max_iter=1500, verbose=True, tol=0.0000100, solver='adam', \n",
    "                                    activation='relu', hidden_layer_sizes=(2,2))\n",
    "rede_neural_credit.fit(x_credit_treinamento, y_credit_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes = rede_neural_credit.predict(x_credit_teste)\n",
    "previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_credit_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.946"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "accuracy_score(y_credit_teste, previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.946"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "cm = ConfusionMatrix(rede_neural_credit)\n",
    "cm.fit(x_credit_treinamento, y_credit_treinamento)\n",
    "cm.score(x_credit_teste, y_credit_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base Census - 94,6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../census.pkl', 'rb') as f:\n",
    "    x_census_treinamento, y_census_treinamento, x_census_teste, y_census_teste = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27676, 108), (27676,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_census_treinamento.shape, y_census_treinamento.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4885, 108), (4885,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_census_teste.shape, y_census_teste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.43623459\n",
      "Iteration 2, loss = 0.33138370\n",
      "Iteration 3, loss = 0.31839411\n",
      "Iteration 4, loss = 0.31037889\n",
      "Iteration 5, loss = 0.30520303\n",
      "Iteration 6, loss = 0.30095165\n",
      "Iteration 7, loss = 0.29750359\n",
      "Iteration 8, loss = 0.29476393\n",
      "Iteration 9, loss = 0.29219845\n",
      "Iteration 10, loss = 0.28978115\n",
      "Iteration 11, loss = 0.28723521\n",
      "Iteration 12, loss = 0.28526705\n",
      "Iteration 13, loss = 0.28411228\n",
      "Iteration 14, loss = 0.28210205\n",
      "Iteration 15, loss = 0.28061729\n",
      "Iteration 16, loss = 0.27893531\n",
      "Iteration 17, loss = 0.27650091\n",
      "Iteration 18, loss = 0.27578743\n",
      "Iteration 19, loss = 0.27346229\n",
      "Iteration 20, loss = 0.27106912\n",
      "Iteration 21, loss = 0.26944120\n",
      "Iteration 22, loss = 0.26839667\n",
      "Iteration 23, loss = 0.26671627\n",
      "Iteration 24, loss = 0.26419346\n",
      "Iteration 25, loss = 0.26334157\n",
      "Iteration 26, loss = 0.26184952\n",
      "Iteration 27, loss = 0.26023212\n",
      "Iteration 28, loss = 0.25980777\n",
      "Iteration 29, loss = 0.25692332\n",
      "Iteration 30, loss = 0.25511038\n",
      "Iteration 31, loss = 0.25393780\n",
      "Iteration 32, loss = 0.25213387\n",
      "Iteration 33, loss = 0.25101229\n",
      "Iteration 34, loss = 0.24981622\n",
      "Iteration 35, loss = 0.24884740\n",
      "Iteration 36, loss = 0.24716968\n",
      "Iteration 37, loss = 0.24618979\n",
      "Iteration 38, loss = 0.24486989\n",
      "Iteration 39, loss = 0.24303853\n",
      "Iteration 40, loss = 0.24175411\n",
      "Iteration 41, loss = 0.24068664\n",
      "Iteration 42, loss = 0.23910984\n",
      "Iteration 43, loss = 0.23882405\n",
      "Iteration 44, loss = 0.23774265\n",
      "Iteration 45, loss = 0.23676418\n",
      "Iteration 46, loss = 0.23538068\n",
      "Iteration 47, loss = 0.23349329\n",
      "Iteration 48, loss = 0.23299275\n",
      "Iteration 49, loss = 0.23206751\n",
      "Iteration 50, loss = 0.23105411\n",
      "Iteration 51, loss = 0.22990463\n",
      "Iteration 52, loss = 0.22925516\n",
      "Iteration 53, loss = 0.22732501\n",
      "Iteration 54, loss = 0.22719205\n",
      "Iteration 55, loss = 0.22608612\n",
      "Iteration 56, loss = 0.22490909\n",
      "Iteration 57, loss = 0.22437308\n",
      "Iteration 58, loss = 0.22281928\n",
      "Iteration 59, loss = 0.22246475\n",
      "Iteration 60, loss = 0.22068787\n",
      "Iteration 61, loss = 0.22079400\n",
      "Iteration 62, loss = 0.21986932\n",
      "Iteration 63, loss = 0.21947917\n",
      "Iteration 64, loss = 0.21766486\n",
      "Iteration 65, loss = 0.21693266\n",
      "Iteration 66, loss = 0.21717083\n",
      "Iteration 67, loss = 0.21532169\n",
      "Iteration 68, loss = 0.21552488\n",
      "Iteration 69, loss = 0.21499180\n",
      "Iteration 70, loss = 0.21428520\n",
      "Iteration 71, loss = 0.21168411\n",
      "Iteration 72, loss = 0.21126313\n",
      "Iteration 73, loss = 0.21133978\n",
      "Iteration 74, loss = 0.21040422\n",
      "Iteration 75, loss = 0.20997142\n",
      "Iteration 76, loss = 0.20867360\n",
      "Iteration 77, loss = 0.20880974\n",
      "Iteration 78, loss = 0.20711330\n",
      "Iteration 79, loss = 0.20676935\n",
      "Iteration 80, loss = 0.20640720\n",
      "Iteration 81, loss = 0.20668632\n",
      "Iteration 82, loss = 0.20610420\n",
      "Iteration 83, loss = 0.20649479\n",
      "Iteration 84, loss = 0.20371386\n",
      "Iteration 85, loss = 0.20353076\n",
      "Iteration 86, loss = 0.20243111\n",
      "Iteration 87, loss = 0.20186614\n",
      "Iteration 88, loss = 0.20213240\n",
      "Iteration 89, loss = 0.20143640\n",
      "Iteration 90, loss = 0.19993758\n",
      "Iteration 91, loss = 0.20071835\n",
      "Iteration 92, loss = 0.19934551\n",
      "Iteration 93, loss = 0.19878019\n",
      "Iteration 94, loss = 0.19897587\n",
      "Iteration 95, loss = 0.19856128\n",
      "Iteration 96, loss = 0.19757365\n",
      "Iteration 97, loss = 0.19700871\n",
      "Iteration 98, loss = 0.19664131\n",
      "Iteration 99, loss = 0.19446948\n",
      "Iteration 100, loss = 0.19614409\n",
      "Iteration 101, loss = 0.19462770\n",
      "Iteration 102, loss = 0.19485407\n",
      "Iteration 103, loss = 0.19236531\n",
      "Iteration 104, loss = 0.19398525\n",
      "Iteration 105, loss = 0.19265347\n",
      "Iteration 106, loss = 0.19317120\n",
      "Iteration 107, loss = 0.19148797\n",
      "Iteration 108, loss = 0.19156839\n",
      "Iteration 109, loss = 0.19120691\n",
      "Iteration 110, loss = 0.19048233\n",
      "Iteration 111, loss = 0.19028804\n",
      "Iteration 112, loss = 0.19007244\n",
      "Iteration 113, loss = 0.18830389\n",
      "Iteration 114, loss = 0.18827838\n",
      "Iteration 115, loss = 0.18786847\n",
      "Iteration 116, loss = 0.18813956\n",
      "Iteration 117, loss = 0.18662184\n",
      "Iteration 118, loss = 0.18649457\n",
      "Iteration 119, loss = 0.18590617\n",
      "Iteration 120, loss = 0.18649981\n",
      "Iteration 121, loss = 0.18657647\n",
      "Iteration 122, loss = 0.18651076\n",
      "Iteration 123, loss = 0.18514649\n",
      "Iteration 124, loss = 0.18456590\n",
      "Iteration 125, loss = 0.18329839\n",
      "Iteration 126, loss = 0.18347174\n",
      "Iteration 127, loss = 0.18373645\n",
      "Iteration 128, loss = 0.18320533\n",
      "Iteration 129, loss = 0.18184195\n",
      "Iteration 130, loss = 0.18285509\n",
      "Iteration 131, loss = 0.18131277\n",
      "Iteration 132, loss = 0.18112790\n",
      "Iteration 133, loss = 0.18130852\n",
      "Iteration 134, loss = 0.18014546\n",
      "Iteration 135, loss = 0.18103696\n",
      "Iteration 136, loss = 0.18059356\n",
      "Iteration 137, loss = 0.17909920\n",
      "Iteration 138, loss = 0.17963798\n",
      "Iteration 139, loss = 0.17738709\n",
      "Iteration 140, loss = 0.17779227\n",
      "Iteration 141, loss = 0.17880159\n",
      "Iteration 142, loss = 0.17641269\n",
      "Iteration 143, loss = 0.17699664\n",
      "Iteration 144, loss = 0.17647600\n",
      "Iteration 145, loss = 0.17579018\n",
      "Iteration 146, loss = 0.17521510\n",
      "Iteration 147, loss = 0.17668770\n",
      "Iteration 148, loss = 0.17551331\n",
      "Iteration 149, loss = 0.17430044\n",
      "Iteration 150, loss = 0.17399377\n",
      "Iteration 151, loss = 0.17560197\n",
      "Iteration 152, loss = 0.17447333\n",
      "Iteration 153, loss = 0.17553767\n",
      "Iteration 154, loss = 0.17246017\n",
      "Iteration 155, loss = 0.17282836\n",
      "Iteration 156, loss = 0.17114036\n",
      "Iteration 157, loss = 0.17200257\n",
      "Iteration 158, loss = 0.17170832\n",
      "Iteration 159, loss = 0.17078326\n",
      "Iteration 160, loss = 0.17076193\n",
      "Iteration 161, loss = 0.17018039\n",
      "Iteration 162, loss = 0.17037389\n",
      "Iteration 163, loss = 0.17149839\n",
      "Iteration 164, loss = 0.17033591\n",
      "Iteration 165, loss = 0.16958255\n",
      "Iteration 166, loss = 0.16837331\n",
      "Iteration 167, loss = 0.16918546\n",
      "Iteration 168, loss = 0.16860055\n",
      "Iteration 169, loss = 0.16966611\n",
      "Iteration 170, loss = 0.16782498\n",
      "Iteration 171, loss = 0.16790603\n",
      "Iteration 172, loss = 0.16682711\n",
      "Iteration 173, loss = 0.16726114\n",
      "Iteration 174, loss = 0.16667745\n",
      "Iteration 175, loss = 0.16651999\n",
      "Iteration 176, loss = 0.16664163\n",
      "Iteration 177, loss = 0.16528806\n",
      "Iteration 178, loss = 0.16583268\n",
      "Iteration 179, loss = 0.16437539\n",
      "Iteration 180, loss = 0.16622730\n",
      "Iteration 181, loss = 0.16545161\n",
      "Iteration 182, loss = 0.16449162\n",
      "Iteration 183, loss = 0.16556403\n",
      "Iteration 184, loss = 0.16473055\n",
      "Iteration 185, loss = 0.16392016\n",
      "Iteration 186, loss = 0.16305764\n",
      "Iteration 187, loss = 0.16490513\n",
      "Iteration 188, loss = 0.16282995\n",
      "Iteration 189, loss = 0.16282665\n",
      "Iteration 190, loss = 0.16281231\n",
      "Iteration 191, loss = 0.16128142\n",
      "Iteration 192, loss = 0.16178751\n",
      "Iteration 193, loss = 0.16153135\n",
      "Iteration 194, loss = 0.16155120\n",
      "Iteration 195, loss = 0.16139844\n",
      "Iteration 196, loss = 0.16119911\n",
      "Iteration 197, loss = 0.15948627\n",
      "Iteration 198, loss = 0.16124326\n",
      "Iteration 199, loss = 0.16154726\n",
      "Iteration 200, loss = 0.15903896\n",
      "Iteration 201, loss = 0.15883253\n",
      "Iteration 202, loss = 0.16265947\n",
      "Iteration 203, loss = 0.15977481\n",
      "Iteration 204, loss = 0.16048969\n",
      "Iteration 205, loss = 0.15806978\n",
      "Iteration 206, loss = 0.15924466\n",
      "Iteration 207, loss = 0.15951211\n",
      "Iteration 208, loss = 0.15906229\n",
      "Iteration 209, loss = 0.15853858\n",
      "Iteration 210, loss = 0.15854450\n",
      "Iteration 211, loss = 0.15901571\n",
      "Iteration 212, loss = 0.15713126\n",
      "Iteration 213, loss = 0.15696506\n",
      "Iteration 214, loss = 0.15879324\n",
      "Iteration 215, loss = 0.15747144\n",
      "Iteration 216, loss = 0.15817246\n",
      "Iteration 217, loss = 0.15517615\n",
      "Iteration 218, loss = 0.15520566\n",
      "Iteration 219, loss = 0.15483627\n",
      "Iteration 220, loss = 0.15570463\n",
      "Iteration 221, loss = 0.15510452\n",
      "Iteration 222, loss = 0.15601984\n",
      "Iteration 223, loss = 0.15598593\n",
      "Iteration 224, loss = 0.15460176\n",
      "Iteration 225, loss = 0.15470575\n",
      "Iteration 226, loss = 0.15482607\n",
      "Iteration 227, loss = 0.15530427\n",
      "Iteration 228, loss = 0.15464834\n",
      "Iteration 229, loss = 0.15297073\n",
      "Iteration 230, loss = 0.15420913\n",
      "Iteration 231, loss = 0.15191820\n",
      "Iteration 232, loss = 0.15621355\n",
      "Iteration 233, loss = 0.15300674\n",
      "Iteration 234, loss = 0.15357487\n",
      "Iteration 235, loss = 0.15337828\n",
      "Iteration 236, loss = 0.15498927\n",
      "Iteration 237, loss = 0.15320728\n",
      "Iteration 238, loss = 0.15272643\n",
      "Iteration 239, loss = 0.15284696\n",
      "Iteration 240, loss = 0.15097745\n",
      "Iteration 241, loss = 0.15144810\n",
      "Iteration 242, loss = 0.15091032\n",
      "Iteration 243, loss = 0.15197976\n",
      "Iteration 244, loss = 0.15216959\n",
      "Iteration 245, loss = 0.15113578\n",
      "Iteration 246, loss = 0.15110555\n",
      "Iteration 247, loss = 0.15132497\n",
      "Iteration 248, loss = 0.15124501\n",
      "Iteration 249, loss = 0.15013308\n",
      "Iteration 250, loss = 0.14943087\n",
      "Iteration 251, loss = 0.14870955\n",
      "Iteration 252, loss = 0.14841048\n",
      "Iteration 253, loss = 0.14942152\n",
      "Iteration 254, loss = 0.14925566\n",
      "Iteration 255, loss = 0.14890738\n",
      "Iteration 256, loss = 0.14951250\n",
      "Iteration 257, loss = 0.14995309\n",
      "Iteration 258, loss = 0.14972353\n",
      "Iteration 259, loss = 0.14708787\n",
      "Iteration 260, loss = 0.14774102\n",
      "Iteration 261, loss = 0.14863518\n",
      "Iteration 262, loss = 0.14935761\n",
      "Iteration 263, loss = 0.14703616\n",
      "Iteration 264, loss = 0.14739007\n",
      "Iteration 265, loss = 0.14754597\n",
      "Iteration 266, loss = 0.14714630\n",
      "Iteration 267, loss = 0.14646711\n",
      "Iteration 268, loss = 0.14773613\n",
      "Iteration 269, loss = 0.14677938\n",
      "Iteration 270, loss = 0.14625841\n",
      "Iteration 271, loss = 0.14592728\n",
      "Iteration 272, loss = 0.14578235\n",
      "Iteration 273, loss = 0.14543834\n",
      "Iteration 274, loss = 0.14719072\n",
      "Iteration 275, loss = 0.14570790\n",
      "Iteration 276, loss = 0.14571399\n",
      "Iteration 277, loss = 0.14703591\n",
      "Iteration 278, loss = 0.14663938\n",
      "Iteration 279, loss = 0.14490397\n",
      "Iteration 280, loss = 0.14622930\n",
      "Iteration 281, loss = 0.14584017\n",
      "Iteration 282, loss = 0.14427750\n",
      "Iteration 283, loss = 0.14466454\n",
      "Iteration 284, loss = 0.14425597\n",
      "Iteration 285, loss = 0.14373951\n",
      "Iteration 286, loss = 0.14491332\n",
      "Iteration 287, loss = 0.14407611\n",
      "Iteration 288, loss = 0.14424029\n",
      "Iteration 289, loss = 0.14242077\n",
      "Iteration 290, loss = 0.14325575\n",
      "Iteration 291, loss = 0.14290955\n",
      "Iteration 292, loss = 0.14263181\n",
      "Iteration 293, loss = 0.14144186\n",
      "Iteration 294, loss = 0.14343719\n",
      "Iteration 295, loss = 0.14179631\n",
      "Iteration 296, loss = 0.14348985\n",
      "Iteration 297, loss = 0.14267293\n",
      "Iteration 298, loss = 0.14186819\n",
      "Iteration 299, loss = 0.14176151\n",
      "Iteration 300, loss = 0.14212791\n",
      "Iteration 301, loss = 0.14093300\n",
      "Iteration 302, loss = 0.14126590\n",
      "Iteration 303, loss = 0.14095823\n",
      "Iteration 304, loss = 0.14025648\n",
      "Iteration 305, loss = 0.13988817\n",
      "Iteration 306, loss = 0.14192861\n",
      "Iteration 307, loss = 0.14152528\n",
      "Iteration 308, loss = 0.14088347\n",
      "Iteration 309, loss = 0.14001714\n",
      "Iteration 310, loss = 0.14265389\n",
      "Iteration 311, loss = 0.14050578\n",
      "Iteration 312, loss = 0.14134994\n",
      "Iteration 313, loss = 0.13924628\n",
      "Iteration 314, loss = 0.13989639\n",
      "Iteration 315, loss = 0.13950210\n",
      "Iteration 316, loss = 0.13990412\n",
      "Iteration 317, loss = 0.14070213\n",
      "Iteration 318, loss = 0.13806934\n",
      "Iteration 319, loss = 0.13944099\n",
      "Iteration 320, loss = 0.13846274\n",
      "Iteration 321, loss = 0.14093979\n",
      "Iteration 322, loss = 0.13851031\n",
      "Iteration 323, loss = 0.13926577\n",
      "Iteration 324, loss = 0.13869447\n",
      "Iteration 325, loss = 0.13872142\n",
      "Iteration 326, loss = 0.13754758\n",
      "Iteration 327, loss = 0.13813846\n",
      "Iteration 328, loss = 0.13908128\n",
      "Iteration 329, loss = 0.13842139\n",
      "Iteration 330, loss = 0.13824848\n",
      "Iteration 331, loss = 0.13954284\n",
      "Iteration 332, loss = 0.13686099\n",
      "Iteration 333, loss = 0.13676683\n",
      "Iteration 334, loss = 0.13667819\n",
      "Iteration 335, loss = 0.13923849\n",
      "Iteration 336, loss = 0.13868215\n",
      "Iteration 337, loss = 0.13674555\n",
      "Iteration 338, loss = 0.13874088\n",
      "Iteration 339, loss = 0.13651592\n",
      "Iteration 340, loss = 0.13667638\n",
      "Iteration 341, loss = 0.13602371\n",
      "Iteration 342, loss = 0.13512689\n",
      "Iteration 343, loss = 0.13589721\n",
      "Iteration 344, loss = 0.13668650\n",
      "Iteration 345, loss = 0.13579080\n",
      "Iteration 346, loss = 0.13671809\n",
      "Iteration 347, loss = 0.13481029\n",
      "Iteration 348, loss = 0.13501291\n",
      "Iteration 349, loss = 0.13631359\n",
      "Iteration 350, loss = 0.13629403\n",
      "Iteration 351, loss = 0.13552149\n",
      "Iteration 352, loss = 0.13538966\n",
      "Iteration 353, loss = 0.13583155\n",
      "Iteration 354, loss = 0.13513779\n",
      "Iteration 355, loss = 0.13442779\n",
      "Iteration 356, loss = 0.13802365\n",
      "Iteration 357, loss = 0.13391690\n",
      "Iteration 358, loss = 0.13516776\n",
      "Iteration 359, loss = 0.13476803\n",
      "Iteration 360, loss = 0.13299287\n",
      "Iteration 361, loss = 0.13335946\n",
      "Iteration 362, loss = 0.13440286\n",
      "Iteration 363, loss = 0.13357289\n",
      "Iteration 364, loss = 0.13463080\n",
      "Iteration 365, loss = 0.13387822\n",
      "Iteration 366, loss = 0.13640297\n",
      "Iteration 367, loss = 0.13345469\n",
      "Iteration 368, loss = 0.13142145\n",
      "Iteration 369, loss = 0.13226162\n",
      "Iteration 370, loss = 0.13453663\n",
      "Iteration 371, loss = 0.13347247\n",
      "Iteration 372, loss = 0.13256323\n",
      "Iteration 373, loss = 0.13338967\n",
      "Iteration 374, loss = 0.13325417\n",
      "Iteration 375, loss = 0.13457957\n",
      "Iteration 376, loss = 0.13378147\n",
      "Iteration 377, loss = 0.13237706\n",
      "Iteration 378, loss = 0.13425777\n",
      "Iteration 379, loss = 0.13137137\n",
      "Iteration 380, loss = 0.13307690\n",
      "Iteration 381, loss = 0.13280637\n",
      "Iteration 382, loss = 0.13388091\n",
      "Iteration 383, loss = 0.13262286\n",
      "Iteration 384, loss = 0.13095116\n",
      "Iteration 385, loss = 0.13029266\n",
      "Iteration 386, loss = 0.13043439\n",
      "Iteration 387, loss = 0.12912582\n",
      "Iteration 388, loss = 0.13098294\n",
      "Iteration 389, loss = 0.13031840\n",
      "Iteration 390, loss = 0.13615904\n",
      "Iteration 391, loss = 0.13082453\n",
      "Iteration 392, loss = 0.12954278\n",
      "Iteration 393, loss = 0.13084116\n",
      "Iteration 394, loss = 0.12905032\n",
      "Iteration 395, loss = 0.13021020\n",
      "Iteration 396, loss = 0.13024697\n",
      "Iteration 397, loss = 0.12936573\n",
      "Iteration 398, loss = 0.13025269\n",
      "Iteration 399, loss = 0.12904060\n",
      "Iteration 400, loss = 0.13070938\n",
      "Iteration 401, loss = 0.12792428\n",
      "Iteration 402, loss = 0.13012642\n",
      "Iteration 403, loss = 0.12852197\n",
      "Iteration 404, loss = 0.13075770\n",
      "Iteration 405, loss = 0.13012309\n",
      "Iteration 406, loss = 0.13007753\n",
      "Iteration 407, loss = 0.13083446\n",
      "Iteration 408, loss = 0.13069257\n",
      "Iteration 409, loss = 0.13013448\n",
      "Iteration 410, loss = 0.12837216\n",
      "Iteration 411, loss = 0.12756460\n",
      "Iteration 412, loss = 0.12759106\n",
      "Iteration 413, loss = 0.12742371\n",
      "Iteration 414, loss = 0.12839232\n",
      "Iteration 415, loss = 0.13087886\n",
      "Iteration 416, loss = 0.12712070\n",
      "Iteration 417, loss = 0.12775720\n",
      "Iteration 418, loss = 0.12851809\n",
      "Iteration 419, loss = 0.12761314\n",
      "Iteration 420, loss = 0.12855773\n",
      "Iteration 421, loss = 0.12660696\n",
      "Iteration 422, loss = 0.12758089\n",
      "Iteration 423, loss = 0.12939226\n",
      "Iteration 424, loss = 0.12810362\n",
      "Iteration 425, loss = 0.13211040\n",
      "Iteration 426, loss = 0.12868579\n",
      "Iteration 427, loss = 0.12635547\n",
      "Iteration 428, loss = 0.12725892\n",
      "Iteration 429, loss = 0.12796035\n",
      "Iteration 430, loss = 0.12856997\n",
      "Iteration 431, loss = 0.13048084\n",
      "Iteration 432, loss = 0.12508662\n",
      "Iteration 433, loss = 0.12739315\n",
      "Iteration 434, loss = 0.12725357\n",
      "Iteration 435, loss = 0.12753908\n",
      "Iteration 436, loss = 0.12732755\n",
      "Iteration 437, loss = 0.12995778\n",
      "Iteration 438, loss = 0.12627406\n",
      "Iteration 439, loss = 0.12719265\n",
      "Iteration 440, loss = 0.12708244\n",
      "Iteration 441, loss = 0.12562492\n",
      "Iteration 442, loss = 0.12650027\n",
      "Iteration 443, loss = 0.12632784\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1500, tol=1e-05,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rede_neural_census = MLPClassifier(max_iter=1500, verbose=True, tol=0.0000100, \n",
    "                                    hidden_layer_sizes=(55,55))\n",
    "rede_neural_census.fit(x_census_treinamento, y_census_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes = rede_neural_credit.predict(x_credit_teste)\n",
    "previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.946"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "accuracy_score(y_credit_teste, previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.946"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "cm = ConfusionMatrix(rede_neural_credit)\n",
    "cm.fit(x_credit_treinamento, y_credit_treinamento)\n",
    "cm.score(x_credit_teste, y_credit_teste)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "552d2b7b9e9e4ed5a1d94a67e1ddcd9de1a38556c9f3135074669235b245b1d8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('machine': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
